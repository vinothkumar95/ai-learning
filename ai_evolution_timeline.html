<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Epic Journey to "Attention is All You Need"</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .header {
            text-align: center;
            color: white;
            margin-bottom: 40px;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .timeline {
            position: relative;
            padding: 20px 0;
        }
        
        .timeline::before {
            content: '';
            position: absolute;
            left: 50%;
            top: 0;
            bottom: 0;
            width: 4px;
            background: linear-gradient(to bottom, #ff6b6b, #4ecdc4, #45b7d1, #96ceb4, #feca57);
            transform: translateX(-50%);
        }
        
        .milestone {
            position: relative;
            margin: 50px 0;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .milestone:hover {
            transform: scale(1.02);
        }
        
        .milestone-content {
            background: white;
            border-radius: 15px;
            padding: 25px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            width: 45%;
            position: relative;
            transition: all 0.3s ease;
        }
        
        .milestone:nth-child(even) .milestone-content {
            margin-left: 55%;
        }
        
        .milestone-content:hover {
            box-shadow: 0 20px 40px rgba(0,0,0,0.15);
        }
        
        .milestone-year {
            position: absolute;
            left: 50%;
            top: 20px;
            transform: translateX(-50%);
            background: #333;
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            z-index: 10;
        }
        
        .milestone-icon {
            position: absolute;
            left: 50%;
            top: 15px;
            transform: translateX(-50%);
            width: 60px;
            height: 60px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 24px;
            z-index: 10;
            border: 4px solid white;
        }
        
        .perceptron { background: #ff6b6b; }
        .mlp { background: #4ecdc4; }
        .cnn { background: #45b7d1; }
        .rnn { background: #96ceb4; }
        .lstm { background: #feca57; }
        .seq2seq { background: #fd79a8; }
        .attention { background: #a29bfe; }
        .transformer { background: #00b894; }
        
        .milestone h3 {
            color: #333;
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        
        .milestone-description {
            line-height: 1.6;
            margin-bottom: 15px;
        }
        
        .breakthrough {
            background: linear-gradient(45deg, #ffd700, #ffed4e);
            padding: 10px;
            border-radius: 8px;
            font-weight: bold;
            margin: 10px 0;
            border-left: 4px solid #ff6b6b;
        }
        
        .tech-details {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            border-left: 4px solid #45b7d1;
            display: none;
        }
        
        .show-details {
            background: #45b7d1;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 20px;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .show-details:hover {
            background: #357abd;
            transform: translateY(-2px);
        }
        
        .final-reveal {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-align: center;
            padding: 40px;
            border-radius: 20px;
            margin: 50px 0;
            box-shadow: 0 20px 40px rgba(0,0,0,0.2);
        }
        
        .final-reveal h2 {
            font-size: 2.2em;
            margin-bottom: 20px;
        }
        
        .dramatic-text {
            font-size: 1.3em;
            line-height: 1.8;
            font-style: italic;
        }
        
        @media (max-width: 768px) {
            .timeline::before {
                left: 30px;
            }
            
            .milestone-content {
                width: calc(100% - 80px);
                margin-left: 80px !important;
            }
            
            .milestone-year {
                left: 30px;
            }
            
            .milestone-icon {
                left: 30px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üß† The Epic Journey to "Attention is All You Need"</h1>
            <p>From humble beginnings to revolutionary breakthroughs - the story of AI evolution</p>
        </div>
        
        <div class="timeline">
            <div class="milestone">
                <div class="milestone-year">1943</div>
                <div class="milestone-icon perceptron">üßÆ</div>
                <div class="milestone-content">
                    <h3>The Perceptron - The First Spark</h3>
                    <p class="milestone-description">
                        McCulloch & Pitts created the first mathematical model of a neuron. Think of it as the "Hello World" of AI - incredibly simple but revolutionary.
                    </p>
                    <div class="breakthrough">
                        üí° Breakthrough: Proved that simple binary operations could perform logical reasoning!
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Code Behind the Magic:</strong><br>
                        ‚Ä¢ Input: Binary values (0 or 1)<br>
                        ‚Ä¢ Process: Weighted sum + threshold function<br>
                        ‚Ä¢ Output: Fire (1) or don't fire (0)<br>
                        ‚Ä¢ Limitation: Could only solve linearly separable problems (like AND, OR, but not XOR)
                    </div>
                </div>
            </div>
            
            <div class="milestone">
                <div class="milestone-year">1986</div>
                <div class="milestone-icon mlp">üï∏Ô∏è</div>
                <div class="milestone-content">
                    <h3>Multi-Layer Perceptrons - The Great Awakening</h3>
                    <p class="milestone-description">
                        Backpropagation changed everything! Suddenly we could train deep networks and solve non-linear problems. The XOR problem that stumped single perceptrons? Solved!
                    </p>
                    <div class="breakthrough">
                        üî• Breakthrough: Networks could now learn complex patterns by adjusting weights backwards through layers!
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Magic Formula:</strong><br>
                        ‚Ä¢ Forward pass: Input ‚Üí Hidden layers ‚Üí Output<br>
                        ‚Ä¢ Backward pass: Calculate error and propagate back<br>
                        ‚Ä¢ Weight update: Use gradient descent to minimize error<br>
                        ‚Ä¢ Game changer: Could approximate any continuous function!
                    </div>
                </div>
            </div>
            
            <div class="milestone">
                <div class="milestone-year">1998</div>
                <div class="milestone-icon cnn">üëÅÔ∏è</div>
                <div class="milestone-content">
                    <h3>Convolutional Neural Networks - Seeing is Believing</h3>
                    <p class="milestone-description">
                        LeCun's CNN (LeNet) taught computers to "see" by recognizing that images have spatial structure. Instead of treating pixels as random numbers, CNNs understand neighborhoods and patterns.
                    </p>
                    <div class="breakthrough">
                        üñºÔ∏è Breakthrough: Finally, computers could recognize handwritten digits reliably!
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Vision Architecture:</strong><br>
                        ‚Ä¢ Convolution: Sliding filters detect features (edges, corners)<br>
                        ‚Ä¢ Pooling: Reduce size while keeping important info<br>
                        ‚Ä¢ Translation invariant: A cat is a cat whether it's top-left or bottom-right<br>
                        ‚Ä¢ Hierarchy: Simple features ‚Üí Complex features
                    </div>
                </div>
            </div>
            
            <div class="milestone">
                <div class="milestone-year">1997</div>
                <div class="milestone-icon rnn">üîÑ</div>
                <div class="milestone-content">
                    <h3>Recurrent Neural Networks - Memory Awakens</h3>
                    <p class="milestone-description">
                        RNNs introduced the concept of memory to neural networks. Unlike previous networks that forgot everything after each input, RNNs could remember what happened before.
                    </p>
                    <div class="breakthrough">
                        üß† Breakthrough: Networks gained the ability to process sequences and maintain context!
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Memory Mechanism:</strong><br>
                        ‚Ä¢ Hidden state carries information from previous time steps<br>
                        ‚Ä¢ h(t) = f(W * x(t) + U * h(t-1) + b)<br>
                        ‚Ä¢ Problem: Vanishing gradients made long-term memory impossible<br>
                        ‚Ä¢ Use cases: Language modeling, time series prediction
                    </div>
                </div>
            </div>
            
            <div class="milestone">
                <div class="milestone-year">1997</div>
                <div class="milestone-icon lstm">‚è≥</div>
                <div class="milestone-content">
                    <h3>LSTM - The Memory Master</h3>
                    <p class="milestone-description">
                        Hochreiter & Schmidhuber solved RNN's memory problem with LSTM. Think of it as giving the network a sophisticated filing system with forget gates, input gates, and output gates.
                    </p>
                    <div class="breakthrough">
                        üéØ Breakthrough: Long-term dependencies finally became learnable!
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Gate Architecture:</strong><br>
                        ‚Ä¢ Forget gate: What to remove from memory<br>
                        ‚Ä¢ Input gate: What new info to store<br>
                        ‚Ä¢ Output gate: What parts of memory to use<br>
                        ‚Ä¢ Cell state: The actual long-term memory highway
                    </div>
                </div>
            </div>
            
            <div class="milestone">
                <div class="milestone-year">2014</div>
                <div class="milestone-icon seq2seq">üîÑ</div>
                <div name="milestone-content">
                    <h3>Sequence-to-Sequence - The Translation Revolution</h3>
                    <p class="milestone-description">
                        Sutskever's seq2seq model was like giving AI the ability to be a universal translator. Encoder-decoder architecture could transform any sequence into any other sequence.
                    </p>
                    <div class="breakthrough">
                        üåç Breakthrough: Machine translation became practically useful for the first time!
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Translation Pipeline:</strong><br>
                        ‚Ä¢ Encoder: Compress input sequence into fixed-size vector<br>
                        ‚Ä¢ Decoder: Generate output sequence from that vector<br>
                        ‚Ä¢ Problem: Fixed-size bottleneck lost information<br>
                        ‚Ä¢ Applications: Translation, summarization, chatbots
                    </div>
                </div>
            </div>
            
            <div class="milestone">
                <div class="milestone-year">2015</div>
                <div class="milestone-icon attention">üëÅÔ∏è</div>
                <div class="milestone-content">
                    <h3>Attention Mechanism - The Game Changer</h3>
                    <p class="milestone-description">
                        Bahdanau introduced attention, solving seq2seq's bottleneck problem. Instead of compressing everything into one vector, attention let the decoder "look back" at any part of the input.
                    </p>
                    <div class="breakthrough">
                        üéØ Breakthrough: No more information bottleneck! The decoder could focus on relevant parts of input.
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Attention Formula:</strong><br>
                        ‚Ä¢ Alignment scores: How much to focus on each input position<br>
                        ‚Ä¢ Context vector: Weighted sum of all encoder hidden states<br>
                        ‚Ä¢ Dynamic focusing: Different parts get attention at different times<br>
                        ‚Ä¢ Interpretable: We could finally see what the model was "looking at"!
                    </div>
                </div>
            </div>
            
            <div class="milestone">
                <div class="milestone-year">2017</div>
                <div class="milestone-icon transformer">‚ö°</div>
                <div class="milestone-content">
                    <h3>"Attention is All You Need" - The Revolution</h3>
                    <p class="milestone-description">
                        Google's Transformer paper dropped the mic. They asked: "What if we got rid of recurrence entirely and used ONLY attention?" The answer changed AI forever.
                    </p>
                    <div class="breakthrough">
                        üöÄ Breakthrough: Parallelizable, more efficient, and incredibly powerful - the foundation of GPT, BERT, and ChatGPT!
                    </div>
                    <button class="show-details" onclick="toggleDetails(this)">Show Technical Details</button>
                    <div class="tech-details">
                        <strong>The Revolutionary Architecture:</strong><br>
                        ‚Ä¢ Self-attention: Every position attends to every other position<br>
                        ‚Ä¢ Multi-head attention: Multiple attention patterns simultaneously<br>
                        ‚Ä¢ Positional encoding: Since no recurrence, inject position info<br>
                        ‚Ä¢ Parallel processing: No more sequential bottleneck!<br>
                        ‚Ä¢ Scalability: Could handle much longer sequences efficiently
                    </div>
                </div>
            </div>
        </div>
        
        <div class="final-reveal">
            <h2>üé≠ The Dramatic Irony</h2>
            <p class="dramatic-text">
                For decades, AI researchers tried to make neural networks more brain-like with complex recurrent connections and memory mechanisms. 
                Then in 2017, Google said "What if we just... pay attention?" and created the architecture that powers the AI revolution we see today.
                <br><br>
                Sometimes the simplest ideas are the most revolutionary. ü§Ø
            </p>
        </div>
    </div>
    
    <script>
        function toggleDetails(button) {
            const details = button.nextElementSibling;
            if (details.style.display === 'none' || details.style.display === '') {
                details.style.display = 'block';
                button.textContent = 'Hide Technical Details';
                button.style.background = '#e74c3c';
            } else {
                details.style.display = 'none';
                button.textContent = 'Show Technical Details';
                button.style.background = '#45b7d1';
            }
        }
        
        // Add scroll animations
        const milestones = document.querySelectorAll('.milestone');
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        });
        
        milestones.forEach(milestone => {
            milestone.style.opacity = '0';
            milestone.style.transform = 'translateY(50px)';
            milestone.style.transition = 'all 0.6s ease';
            observer.observe(milestone);
        });
    </script>
</body>
</html>